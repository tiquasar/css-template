from flask import Flask, request, jsonify
import os
import spacy
import pdfplumber
import pandas as pd
import json
import psycopg2
from psycopg2.extras import Json
from fuzzywuzzy import fuzz
import logging
import threading

app = Flask(__name__)

# Base directory where files are stored
BASE_DIRECTORY = "/path/to/your/files"  # Replace with your actual file directory

# Database connection details
DB_HOST = "localhost"
DB_NAME = "ner_db"
DB_USER = "username"
DB_PASSWORD = "password"

# Path where spaCy models will be saved after incremental training
MODEL_PATH_SM = './trained_spacy_sm'  # Small model path
MODEL_PATH_TRF = './trained_spacy_trf'  # Transformer model path

# Load pre-trained spaCy models or trained models from disk if available
if os.path.exists(MODEL_PATH_SM):
    nlp_sm = spacy.load(MODEL_PATH_SM)  # Load trained small model
else:
    nlp_sm = spacy.load("en_core_web_sm")  # Load small model from spaCy

if os.path.exists(MODEL_PATH_TRF):
    nlp_trf = spacy.load(MODEL_PATH_TRF)  # Load trained transformer model
else:
    nlp_trf = spacy.load("en_core_web_trf")  # Load transformer model from spaCy

# Configure logging
logging.basicConfig(level=logging.INFO)

# Connect to PostgreSQL database
def get_db_connection():
    conn = psycopg2.connect(
        host=DB_HOST,
        database=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD
    )
    return conn

# Form file path dynamically based on file name
def get_file_path(file_name):
    return os.path.join(BASE_DIRECTORY, file_name)

# Extract text from PDF using pdfplumber
def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Extract text from CSV using pandas
def extract_text_from_csv(file_path, text_columns=None):
    df = pd.read_csv(file_path)
    if text_columns:
        text_data = df[text_columns].astype(str).agg(' '.join, axis=1).tolist()
    else:
        string_cols = df.select_dtypes(include=['object']).columns
        text_data = df[string_cols].astype(str).agg(' '.join, axis=1).tolist()
    combined_text = ' '.join(text_data)
    return combined_text

# Extract text from Excel (XLSX) using pandas
def extract_text_from_excel(file_path, text_columns=None, sheet_name=0):
    df = pd.read_excel(file_path, sheet_name=sheet_name)
    if text_columns:
        text_data = df[text_columns].astype(str).agg(' '.join, axis=1).tolist()
    else:
        string_cols = df.select_dtypes(include=['object']).columns
        text_data = df[string_cols].astype(str).agg(' '.join, axis=1).tolist()
    combined_text = ' '.join(text_data)
    return combined_text

# Preprocess text: clean and remove special characters
def preprocess_text(text):
    import re
    text = re.sub(r'\s+', ' ', text)  # Remove multiple spaces
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    return text.strip()

# Perform NER using both spaCy models and combine results
def perform_ner(text):
    doc_sm = nlp_sm(text)
    doc_trf = nlp_trf(text)

    # Extract entities from both models
    entities_sm = [(ent.text, ent.label_) for ent in doc_sm.ents]
    entities_trf = [(ent.text, ent.label_) for ent in doc_trf.ents]

    # Combine the results, preferring transformer results if thereâ€™s overlap
    combined_entities = {ent[0]: ent[1] for ent in entities_trf}
    for ent in entities_sm:
        if ent[0] not in combined_entities:
            combined_entities[ent[0]] = ent[1]

    # Return the combined entity list
    return [(entity, label) for entity, label in combined_entities.items()]

# Insert NER results into PostgreSQL
def store_ner_in_db(file_name_pattern, ner_results):
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        # Insert NER results as JSONB into PostgreSQL
        query = """
        INSERT INTO ner_results (file_name_pattern, ner_results)
        VALUES (%s, %s)
        ON CONFLICT (file_name_pattern) DO NOTHING
        """
        cur.execute(query, (file_name_pattern, Json(ner_results)))
        conn.commit()
    except Exception as e:
        conn.rollback()
        logging.error(f"Error storing NER in DB: {e}")
    finally:
        cur.close()
        conn.close()

# Retrieve NER results from PostgreSQL
def get_ner_from_db(file_name_pattern):
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        # Retrieve NER results based on file pattern
        query = "SELECT ner_results FROM ner_results WHERE file_name_pattern = %s"
        cur.execute(query, (file_name_pattern,))
        result = cur.fetchone()
        return result[0] if result else None
    except Exception as e:
        logging.error(f"Error retrieving NER from DB: {e}")
    finally:
        cur.close()
        conn.close()

# Background task: Incrementally train spaCy model
def retrain_spacy_model(new_data, model_type):
    # Create training data in spaCy format
    TRAINING_DATA = [(new_data['text'], {"entities": new_data['entities']})]

    # Select which model to retrain: small model or transformer
    if model_type == 'sm':
        model_path = MODEL_PATH_SM
        nlp = nlp_sm
    else:
        model_path = MODEL_PATH_TRF
        nlp = nlp_trf

    # Train the spaCy model
    ner = nlp.get_pipe("ner")
    optimizer = nlp.resume_training()
    for itn in range(10):  # 10 iterations of training
        for text, annotations in TRAINING_DATA:
            nlp.update([text], [annotations], drop=0.5, sgd=optimizer)

    # Save the updated model to disk
    nlp.to_disk(model_path)
    logging.info(f"{model_type.upper()} model trained and saved to {model_path}")

# Run background task to train both models
def train_in_background(training_data):
    # Train the small model in background
    threading.Thread(target=retrain_spacy_model, args=(training_data, 'sm')).start()
    # Train the transformer model in background
    threading.Thread(target=retrain_spacy_model, args=(training_data, 'trf')).start()

# Calculate cosine similarity between two spaCy docs (vectors)
def cosine_similarity_spacy(doc1, doc2):
    return doc1.similarity(doc2)

# Calculate the similarity between two NER results using fuzzy matching, cosine similarity, and entity types
def ner_similarity(ner1, ner2):
    matching_entities = 0
    total_entities = max(len(ner1), len(ner2))

    for ent1 in ner1:
        for ent2 in ner2:
            if ent1[1] == ent2[1]:  # Compare entity types (e.g., PERSON to PERSON)
                doc1 = nlp_sm(ent1[0])  # Use the small model for vectorization
                doc2 = nlp_sm(ent2[0])

                # Calculate cosine similarity between entity text vectors
                cosine_sim = cosine_similarity_spacy(doc1, doc2)

                # Use fuzzy matching for partial string similarity
                fuzzy_sim = fuzz.token_sort_ratio(ent1[0], ent2[0])

                # Combine cosine similarity and fuzzy similarity
                similarity_score = 0.7 * cosine_sim + 0.3 * (fuzzy_sim / 100)
                if similarity_score >= 0.9:  # Threshold for matching entities
                    matching_entities += 1

    return (matching_entities / total_entities) * 100 if total_entities > 0 else 0

@app.route('/ner', methods=['POST'])
def ner_api():
    try:
        # Get the file name from the request and form the file path
        file_name = request.json.get('file_name')
        file_path = get_file_path(file_name)
        
        if not os.path.exists(file_path):
            return jsonify({"status": "error", "message": "File not found"}), 400

        # Extract file name pattern (base file name without extension)
        file_name_pattern = os.path.basename(file_path).split('.')[0]

        # Determine file type and extract text accordingly
        file_extension = os.path.splitext(file_path)[1].lower()
        if file_extension == '.pdf':
            text = extract_text_from_pdf(file_path)
        elif file_extension == '.csv':
            text = extract_text_from_csv(file_path)
        elif file_extension in ['.xls', '.xlsx']:
            text = extract_text_from_excel(file_path)
        else:
            return jsonify({"status": "error", "message": "Unsupported file type"}), 400

        # Preprocess the extracted text
        text = preprocess_text(text)
        if not text:
            return jsonify({"status": "error", "message": "Failed to extract text"}), 500

        # Perform NER on the text
        ner_results = perform_ner(text)

        # Log NER results
        logging.info(f"NER Results for {file_name_pattern}: {ner_results}")

        # Check if the file pattern has been processed before
        stored_ner = get_ner_from_db(file_name_pattern)
        
        if stored_ner:
            # Compare the new NER results with the stored ones
            similarity = ner_similarity(stored_ner, ner_results)

            if similarity >= 90:
                return jsonify({"status": "valid", "similarity": similarity}), 200
            else:
                return jsonify({"status": "invalid", "similarity": similarity}), 200
        else:
            # Store the NER results for future comparison in PostgreSQL
            store_ner_in_db(file_name_pattern, ner_results)

            # Train the spaCy model with the new data in the background
            training_data = {
                "text": text,
                "entities": [(ent[0], ent[1], label) for ent, label in ner_results]
            }
            train_in_background(training_data)

            return jsonify({"status": "stored", "message": "New NER result stored and model training started"}), 201

    except Exception as e:
        logging.error(f"Error during NER processing: {str(e)}")
        return jsonify({"status": "error", "message": str(e)}), 500

# Run the Flask app
if __name__ == '__main__':
    app.run(debug=True)
