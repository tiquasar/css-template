from flask import Flask, request, jsonify
import os
import spacy
import pdfplumber
import pandas as pd
import json
import psycopg2
from psycopg2.extras import Json
from fuzzywuzzy import fuzz
import logging
import threading
from spacy.training import Example

app = Flask(__name__)

# Base directory where files are stored
BASE_DIRECTORY = "/path/to/your/files"  # Replace with your actual file directory

# Database connection details
DB_HOST = "localhost"
DB_NAME = "ner_db"
DB_USER = "username"
DB_PASSWORD = "password"

# Path where spaCy models will be saved after incremental training
MODEL_PATH_SM = './trained_spacy_sm'  # Small model path
MODEL_PATH_TRF = './trained_spacy_trf'  # Transformer model path

# Load pre-trained spaCy models or trained models from disk if available
if os.path.exists(MODEL_PATH_SM):
    nlp_sm = spacy.load(MODEL_PATH_SM)  # Load trained small model
else:
    nlp_sm = spacy.load("en_core_web_sm")  # Load small model from spaCy

if os.path.exists(MODEL_PATH_TRF):
    nlp_trf = spacy.load(MODEL_PATH_TRF)  # Load trained transformer model
else:
    nlp_trf = spacy.load("en_core_web_trf")  # Load transformer model from spaCy

# Configure logging
logging.basicConfig(level=logging.INFO)

# Connect to PostgreSQL database
def get_db_connection():
    conn = psycopg2.connect(
        host=DB_HOST,
        database=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD
    )
    return conn

# Form file path dynamically based on file name
def get_file_path(file_name):
    return os.path.join(BASE_DIRECTORY, file_name)

# Extract text from PDF using pdfplumber
def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Extract text from CSV using pandas
def extract_text_from_csv(file_path, text_columns=None):
    df = pd.read_csv(file_path)
    if text_columns:
        text_data = df[text_columns].astype(str).agg(' '.join, axis=1).tolist()
    else:
        string_cols = df.select_dtypes(include=['object']).columns
        text_data = df[string_cols].astype(str).agg(' '.join, axis=1).tolist()
    combined_text = ' '.join(text_data)
    return combined_text

# Extract text from Excel (XLSX) using pandas
def extract_text_from_excel(file_path, text_columns=None, sheet_name=0):
    df = pd.read_excel(file_path, sheet_name=sheet_name)
    if text_columns:
        text_data = df[text_columns].astype(str).agg(' '.join, axis=1).tolist()
    else:
        string_cols = df.select_dtypes(include=['object']).columns
        text_data = df[string_cols].astype(str).agg(' '.join, axis=1).tolist()
    combined_text = ' '.join(text_data)
    return combined_text

# Preprocess text: clean and remove special characters
def preprocess_text(text):
    import re
    text = re.sub(r'\s+', ' ', text)  # Remove multiple spaces
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    return text.strip()

# Perform NER using both spaCy models and combine results
def perform_ner(text):
    doc_sm = nlp_sm(text)
    doc_trf = nlp_trf(text)

    # Extract entities from both models (ensure format with start_char, end_char, label, text)
    entities_sm = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc_sm.ents]
    entities_trf = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc_trf.ents]

    # Combine the results, preferring transformer results if thereâ€™s overlap
    combined_entities = {ent[0]: (ent[1], ent[2], ent[3]) for ent in entities_trf}
    for ent in entities_sm:
        if ent[0] not in combined_entities:
            combined_entities[ent[0]] = (ent[1], ent[2], ent[3])

    # Return the combined entity list (text, label, start_char, end_char)
    return [(entity, label, start, end) for entity, label, start, end in combined_entities.items()]

# Insert NER results into PostgreSQL
def store_ner_in_db(file_name_pattern, ner_results):
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        # Insert NER results as JSONB into PostgreSQL
        query = """
        INSERT INTO ner_results (file_name_pattern, ner_results)
        VALUES (%s, %s)
        ON CONFLICT (file_name_pattern) DO NOTHING
        """
        cur.execute(query, (file_name_pattern, Json(ner_results)))
        conn.commit()
    except Exception as e:
        conn.rollback()
        logging.error(f"Error storing NER in DB: {e}")
    finally:
        cur.close()
        conn.close()

# Retrieve NER results from PostgreSQL
def get_ner_from_db(file_name_pattern):
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        # Retrieve NER results based on file pattern
        query = "SELECT ner_results FROM ner_results WHERE file_name_pattern = %s"
        cur.execute(query, (file_name_pattern,))
        result = cur.fetchone()

        # Ensure the result contains the expected format (text, label, start_char, end_char)
        if result:
            ner_results = result[0]
            for entity in ner_results:
                if len(entity) != 4:
                    raise ValueError(f"NER result {entity} does not have 4 values (expected text, label, start_char, end_char)")
            return ner_results
        else:
            return None
    except Exception as e:
        logging.error(f"Error retrieving NER from DB: {e}")
    finally:
        cur.close()
        conn.close()

# Background task: Incrementally train spaCy model (updated for spaCy 3.x)
def retrain_spacy_model(new_data, model_type):
    # Select which model to retrain: small model or transformer
    if model_type == 'sm':
        model_path = MODEL_PATH_SM
        nlp = nlp_sm
    else:
        model_path = MODEL_PATH_TRF
        nlp = nlp_trf

    # Ensure the entities are formatted as (start_char, end_char, label)
    entities = []
    for entity in new_data['entities']:
        if isinstance(entity, tuple) and len(entity) == 4:
            text, label, start_char, end_char = entity
            entities.append((start_char, end_char, label))
        else:
            raise ValueError(f"Entity {entity} is not in the correct format (text, label, start_char, end_char)")

    # Filter out overlapping entities
    filtered_entities = filter_overlapping_entities(entities)

    # Create training data in spaCy format using Example
    TRAINING_DATA = [(new_data['text'], {"entities": filtered_entities})]
    examples = []

    for text, annotations in TRAINING_DATA:
        doc = nlp.make_doc(text)  # Create a spaCy Doc object
        example = Example.from_dict(doc, annotations)  # Create Example object
        examples.append(example)

    # Train the spaCy model using Example objects
    ner = nlp.get_pipe("ner")
    optimizer = nlp.resume_training()
    
    for itn in range(10):  # 10 iterations of training
        nlp.update(examples, sgd=optimizer, drop=0.5)

    # Save the updated model to disk
    nlp.to_disk(model_path)
    logging.info(f"{model_type.upper()} model trained and saved to {model_path}")

# Run background task to train both models
def train_in_background(training_data):
    # Train the small model in background
    threading.Thread(target=retrain_spacy_model, args=(training_data, 'sm')).start()
    # Train the transformer model in background
    threading.Thread(target=retrain_spacy_model, args=(training_data, 'trf')).start()

# Calculate cosine similarity between two spaCy docs (vectors)
def cosine_similarity_spacy(doc1, doc2):
    return doc1.similarity(doc2)

# Calculate the similarity between two NER results using fuzzy matching, cosine similarity, and entity types
def ner_similarity(ner1, ner2):
    matching_entities = 0
    total_entities = max(len(ner1), len(ner2))

    total_similarity = 0

    # Convert to sets for more robust comparison (ignores order)
    ner1_set = set([(ent[0], ent[1]) for ent in ner1])  # (text, label)
    ner2_set = set([(ent[0], ent[1]) for ent in ner2])

    # Find matching entities in both sets
    common_entities = ner1_set.intersection(ner2_set)

    # Calculate the number of matching entities
    matching_entities = len(common_entities)
    
    # Calculate fuzzy similarity for non-exact matches
    for ent1 in ner1:
        for ent2 in ner2:
            if ent1[1] == ent2[1]:  # Compare entity types (e.g., PERSON to PERSON)
                doc1 = nlp_sm(ent1[0])  # Use the small model for vectorization
                doc2 = nlp_sm(ent2[0])

                # Calculate cosine similarity between entity text vectors (range 0-1)
                cosine_sim = doc1.similarity(doc2)

                # Scale cosine similarity to 0-100
                cosine_sim_scaled = cosine_sim * 100

                # Use fuzzy matching for partial string similarity (range 0-100)
                fuzzy_sim = fuzz.token_sort_ratio(ent1[0], ent2[0])

                # Combine cosine similarity and fuzzy similarity (70% cosine, 30% fuzzy)
                combined_similarity = 0.7 * cosine_sim_scaled + 0.3 * fuzzy_sim

                total_similarity += combined_similarity
                matching_entities += 1

    # Calculate the final similarity percentage
    if total_entities > 0:
        similarity_percentage = (matching_entities / total_entities) * 100
        return similarity_percentage
    else:
        return 0

@app.route('/ner', methods=['POST'])
def ner_api():
    try:
        # Get the file name from the request and form the file path
        file_name = request.json.get('file_name')
        file_path = get_file_path(file_name)
        
        if not os.path.exists(file_path):
            return jsonify({"status": "error", "message": "File not found"}), 400

        # Extract file name pattern (base file name without extension)
        file_name_pattern = os.path.basename(file_path).split('.')[0]

        # Determine file type and extract text accordingly
        file_extension = os.path.splitext(file_path)[1].lower()
        if file_extension == '.pdf':
            text = extract_text_from_pdf(file_path)
        elif file_extension == '.csv':
            text = extract_text_from_csv(file_path)
        elif file_extension in ['.xls', '.xlsx']:
            text = extract_text_from_excel(file_path)
        else:
            return jsonify({"status": "error", "message": "Unsupported file type"}), 400

        # Preprocess the extracted text
        text = preprocess_text(text)
        if not text:
            return jsonify({"status": "error", "message": "Failed to extract text"}), 500

        # Perform NER on the text
        ner_results = perform_ner(text)

        # Log NER results
        logging.info(f"NER Results for {file_name_pattern}: {ner_results}")

        # Check if the file pattern has been processed before
        stored_ner = get_ner_from_db(file_name_pattern)
        
        if stored_ner:
            # Compare the new NER results with the stored ones
            similarity_percentage = ner_similarity(stored_ner, ner_results)

            if similarity_percentage >= 90:
                return jsonify({"status": "valid", "similarity_percentage": similarity_percentage}), 200
            else:
                return jsonify({"status": "invalid", "similarity_percentage": similarity_percentage}), 200
        else:
            # Store the NER results for future comparison in PostgreSQL
            store_ner_in_db(file_name_pattern, ner_results)

            # Train the spaCy model with the new data in the background
            training_data = {
                "text": text,
                "entities": ner_results  # Passing ner_results directly as entities
            }
            train_in_background(training_data)

            return jsonify({"status": "stored", "message": "New NER result stored and model training started"}), 201

    except Exception as e:
        logging.error(f"Error during NER processing: {str(e)}")
        return jsonify({"status": "error", "message": str(e)}), 500

# Run the Flask app
if __name__ == '__main__':
    app.run(debug=True)












def calculate_entity_positions(text, ner_results):
    entities = []
    for entity, label in ner_results:
        start_char = text.find(entity)
        if start_char == -1:
            raise ValueError(f"Entity '{entity}' not found in the text.")
        end_char = start_char + len(entity)
        entities.append((start_char, end_char, label))
    return entities



# Example text and ner_results
text = "PLALPLL GILS is a company. phoenix hold is another company."
ner_results = [('PLALPLL GILS', 'ORG'), ('phoenix hold', 'ORG')]

# Calculate the start_char and end_char for each entity
training_data = {
    "text": text,
    "entities": calculate_entity_positions(text, ner_results)
}

# Example training data now looks like this:
# {
#     "text": "PLALPLL GILS is a company. phoenix hold is another company.",
#     "entities": [(0, 12, 'ORG'), (25, 37, 'ORG')]
# }

# Background task: Incrementally train spaCy model (updated for spaCy 3.x)
def retrain_spacy_model(new_data, model_type):
    if model_type == 'sm':
        model_path = MODEL_PATH_SM
        nlp = nlp_sm
    else:
        model_path = MODEL_PATH_TRF
        nlp = nlp_trf

    # Ensure the entities are formatted as (start_char, end_char, label)
    entities = new_data['entities']

    # Create training data in spaCy format using Example
    TRAINING_DATA = [(new_data['text'], {"entities": entities})]
    examples = []

    for text, annotations in TRAINING_DATA:
        doc = nlp.make_doc(text)  # Create a spaCy Doc object
        example = Example.from_dict(doc, annotations)  # Create Example object
        examples.append(example)

    # Train the spaCy model using Example objects
    ner = nlp.get_pipe("ner")
    optimizer = nlp.resume_training()
    
    for itn in range(10):  # 10 iterations of training
        nlp.update(examples, sgd=optimizer, drop=0.5)

    # Save the updated model to disk
    nlp.to_disk(model_path)
    logging.info(f"{model_type.upper()} model trained and saved to {model_path}")

# Run the background task
train_in_background(training_data)






def filter_overlapping_entities(entities):
    # Sort entities by their start_char position
    entities = sorted(entities, key=lambda x: x[0])
    non_overlapping_entities = []

    # Track the end of the last added entity
    last_end_char = -1

    for start_char, end_char, label in entities:
        # Only add entities that don't overlap with the last entity
        if start_char >= last_end_char:
            non_overlapping_entities.append((start_char, end_char, label))
            last_end_char = end_char

    return non_overlapping_entities

def calculate_entity_positions(text, ner_results):
    entities = []
    for entity, label in ner_results:
        start_char = text.find(entity)
        if start_char == -1:
            raise ValueError(f"Entity '{entity}' not found in the text.")
        end_char = start_char + len(entity)
        entities.append((start_char, end_char, label))
    return filter_overlapping_entities(entities)  # Filter overlapping entities

def retrain_spacy_model(new_data, model_type):
    if model_type == 'sm':
        model_path = MODEL_PATH_SM
        nlp = nlp_sm
    else:
        model_path = MODEL_PATH_TRF
        nlp = nlp_trf

    # Calculate and filter entities for overlaps
    entities = new_data['entities']

    # Create training data in spaCy format using Example
    TRAINING_DATA = [(new_data['text'], {"entities": entities})]
    examples = []

    for text, annotations in TRAINING_DATA:
        doc = nlp.make_doc(text)  # Create a spaCy Doc object
        example = Example.from_dict(doc, annotations)  # Create Example object
        examples.append(example)

    # Train the spaCy model using Example objects
    ner = nlp.get_pipe("ner")
    optimizer = nlp.resume_training()
    
    for itn in range(10):  # 10 iterations of training
        nlp.update(examples, sgd=optimizer, drop=0.5)

    # Save the updated model to disk
    nlp.to_disk(model_path)
    logging.info(f"{model_type.upper()} model trained and saved to {model_path}")







I have an Excel file containing sample data for reference, a template JSON with placeholders in the format ${xyz} (e.g., ${abc}, ${def}), and a requirement JSON where these placeholders map to specific values or dynamic data. For example, if the requirement JSON specifies ${isin} should be a 13-digit alphanumeric string starting with "ISSU," and the Excel file provides an ISIN example like isin: ISSU1250ad3h9, you can use this as a reference to generate a new ISIN value that adheres to similar standards. However, if the Excel file does not contain relevant data for certain placeholders, feel free to generate the required values based on the description in the requirement JSON. Your task is to read the sample data from the Excel file, replace the placeholders in the template JSON with corresponding values from the requirement JSON or generate them based on the reference, and dynamically generate any missing values. The final output should be a single JSON file where all placeholders have been replaced appropriately. Provide the updated JSON as the output.
