from flask import Flask, request, jsonify
import os
import spacy
import pdfplumber
import pandas as pd
import json
import psycopg2
from psycopg2.extras import Json
from fuzzywuzzy import fuzz
import logging
import threading
from spacy.training import Example

app = Flask(__name__)

# Base directory where files are stored
BASE_DIRECTORY = "/path/to/your/files"  # Replace with your actual file directory

# Database connection details
DB_HOST = "localhost"
DB_NAME = "ner_db"
DB_USER = "username"
DB_PASSWORD = "password"

# Path where spaCy models will be saved after incremental training
MODEL_PATH_SM = './trained_spacy_sm'  # Small model path
MODEL_PATH_TRF = './trained_spacy_trf'  # Transformer model path

# Load pre-trained spaCy models or trained models from disk if available
if os.path.exists(MODEL_PATH_SM):
    nlp_sm = spacy.load(MODEL_PATH_SM)  # Load trained small model
else:
    nlp_sm = spacy.load("en_core_web_sm")  # Load small model from spaCy

if os.path.exists(MODEL_PATH_TRF):
    nlp_trf = spacy.load(MODEL_PATH_TRF)  # Load trained transformer model
else:
    nlp_trf = spacy.load("en_core_web_trf")  # Load transformer model from spaCy

# Configure logging
logging.basicConfig(level=logging.INFO)

# Connect to PostgreSQL database
def get_db_connection():
    conn = psycopg2.connect(
        host=DB_HOST,
        database=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD
    )
    return conn

# Form file path dynamically based on file name
def get_file_path(file_name):
    return os.path.join(BASE_DIRECTORY, file_name)

# Extract text from PDF using pdfplumber
def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Extract text from CSV using pandas
def extract_text_from_csv(file_path, text_columns=None):
    df = pd.read_csv(file_path)
    if text_columns:
        text_data = df[text_columns].astype(str).agg(' '.join, axis=1).tolist()
    else:
        string_cols = df.select_dtypes(include=['object']).columns
        text_data = df[string_cols].astype(str).agg(' '.join, axis=1).tolist()
    combined_text = ' '.join(text_data)
    return combined_text

# Extract text from Excel (XLSX) using pandas
def extract_text_from_excel(file_path, text_columns=None, sheet_name=0):
    df = pd.read_excel(file_path, sheet_name=sheet_name)
    if text_columns:
        text_data = df[text_columns].astype(str).agg(' '.join, axis=1).tolist()
    else:
        string_cols = df.select_dtypes(include=['object']).columns
        text_data = df[string_cols].astype(str).agg(' '.join, axis=1).tolist()
    combined_text = ' '.join(text_data)
    return combined_text

# Preprocess text: clean and remove special characters
def preprocess_text(text):
    import re
    text = re.sub(r'\s+', ' ', text)  # Remove multiple spaces
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    return text.strip()

# Helper function to check if two entities overlap
def entities_overlap(start1, end1, start2, end2):
    return max(start1, start2) < min(end1, end2)

# Function to filter out overlapping entities
def filter_overlapping_entities(entities):
    non_overlapping_entities = []
    for i, (start1, end1, label1) in enumerate(entities):
        overlap = False
        for j, (start2, end2, label2) in enumerate(entities):
            if i != j and entities_overlap(start1, end1, start2, end2):
                overlap = True
                break
        if not overlap:
            non_overlapping_entities.append((start1, end1, label1))
    return non_overlapping_entities

# Perform NER using both spaCy models and combine results
def perform_ner(text):
    doc_sm = nlp_sm(text)
    doc_trf = nlp_trf(text)

    # Extract entities from both models
    entities_sm = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc_sm.ents]
    entities_trf = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc_trf.ents]

    # Combine the results, preferring transformer results if thereâ€™s overlap
    combined_entities = {ent[0]: (ent[1], ent[2], ent[3]) for ent in entities_trf}
    for ent in entities_sm:
        if ent[0] not in combined_entities:
            combined_entities[ent[0]] = (ent[1], ent[2], ent[3])

    # Return the combined entity list
    return [(entity, label, start, end) for entity, label, start, end in combined_entities.items()]

# Insert NER results into PostgreSQL
def store_ner_in_db(file_name_pattern, ner_results):
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        # Insert NER results as JSONB into PostgreSQL
        query = """
        INSERT INTO ner_results (file_name_pattern, ner_results)
        VALUES (%s, %s)
        ON CONFLICT (file_name_pattern) DO NOTHING
        """
        cur.execute(query, (file_name_pattern, Json(ner_results)))
        conn.commit()
    except Exception as e:
        conn.rollback()
        logging.error(f"Error storing NER in DB: {e}")
    finally:
        cur.close()
        conn.close()

# Retrieve NER results from PostgreSQL
def get_ner_from_db(file_name_pattern):
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        # Retrieve NER results based on file pattern
        query = "SELECT ner_results FROM ner_results WHERE file_name_pattern = %s"
        cur.execute(query, (file_name_pattern,))
        result = cur.fetchone()
        return result[0] if result else None
    except Exception as e:
        logging.error(f"Error retrieving NER from DB: {e}")
    finally:
        cur.close()
        conn.close()

# Background task: Incrementally train spaCy model (updated for spaCy 3.x)
def retrain_spacy_model(new_data, model_type):
    # Select which model to retrain: small model or transformer
    if model_type == 'sm':
        model_path = MODEL_PATH_SM
        nlp = nlp_sm
    else:
        model_path = MODEL_PATH_TRF
        nlp = nlp_trf

    # Ensure the entities are formatted as (start_char, end_char, label)
    entities = []
    for entity in new_data['entities']:
        if isinstance(entity, tuple) and len(entity) == 4:
            text, label, start_char, end_char = entity
            entities.append((start_char, end_char, label))
        else:
            raise ValueError(f"Entity {entity} is not in the correct format (text, label, start_char, end_char)")

    # Filter out overlapping entities
    filtered_entities = filter_overlapping_entities(entities)

    # Create training data in spaCy format using Example
    TRAINING_DATA = [(new_data['text'], {"entities": filtered_entities})]
    examples = []

    for text, annotations in TRAINING_DATA:
        doc = nlp.make_doc(text)  # Create a spaCy Doc object
        example = Example.from_dict(doc, annotations)  # Create Example object
        examples.append(example)

    # Train the spaCy model using Example objects
    ner = nlp.get_pipe("ner")
    optimizer = nlp.resume_training()
    
    for itn in range(10):  # 10 iterations of training
        nlp.update(examples, sgd=optimizer, drop=0.5)

    # Save the updated model to disk
    nlp.to_disk(model_path)
    logging.info(f"{model_type.upper()} model trained and saved to {model_path}")

# Run background task to train both models
def train_in_background(training_data):
    # Train the small model in background
    threading.Thread(target=retrain_spacy_model, args=(training_data, 'sm')).start()
    # Train the transformer model in background
    threading.Thread(target=retrain_spacy_model, args=(training_data, 'trf')).start()

# Calculate cosine similarity between two spaCy docs (vectors)
def cosine_similarity_spacy(doc1, doc2):
    return doc1.similarity(doc2)

# Calculate the similarity between two NER results using fuzzy matching, cosine similarity, and entity types
def ner_similarity(ner1, ner2):
    matching_entities = 0
    total_entities = max(len(ner1), len(ner2))

    total_similarity = 0

    # Convert to sets for more robust comparison (ignores order)
    ner1_set = set([(ent[0], ent[1]) for ent in ner1])  # (text, label)
    ner2_set = set([(ent[0], ent[1]) for ent in ner2])

    # Find matching entities in both sets
    common_entities = ner1_set.intersection(ner2_set)

    # Calculate the number of matching entities
    matching_entities = len(common_entities)
    
    # Calculate fuzzy similarity for non-exact matches
    for ent1 in ner1:
        for ent2 in ner2:
            if ent1[1] == ent2[1]:  # Compare entity types (e.g., PERSON to PERSON)
                doc1 = nlp_sm(ent1[0])  # Use the small model for vectorization
                doc2 = nlp_sm(ent2[0])

                # Calculate cosine similarity between entity text vectors (range 0-1)
                cosine_sim = doc1.similarity(doc2)

                # Scale cosine similarity to 0-100
                cosine_sim_scaled = cosine_sim * 100

                # Use fuzzy matching for partial string similarity (range 0-100)
                fuzzy_sim = fuzz.token_sort_ratio(ent1[0], ent2[0])

                # Combine cosine similarity and fuzzy similarity (70% cosine, 30% fuzzy)
                combined_similarity = 0.7 * cosine_sim_scaled + 0.3 * fuzzy_sim

                total_similarity += combined_similarity
                matching_entities += 1

    # Calculate the final similarity percentage
    if total_entities > 0:
        similarity_percentage = (matching_entities / total_entities) * 100
        return similarity_percentage
    else:
        return 0

@app.route('/ner', methods=['POST'])
def ner_api():
    try:
        # Get the file name from the request and form the file path
        file_name = request.json.get('file_name')
        file_path = get_file_path(file_name)
        
        if not os.path.exists(file_path):
            return jsonify({"status": "error", "message": "File not found"}), 400

        # Extract file name pattern (base file name without extension)
        file_name_pattern = os.path.basename(file_path).split('.')[0]

        # Determine file type and extract text accordingly
        file_extension = os.path.splitext(file_path)[1].lower()
        if file_extension == '.pdf':
            text = extract_text_from_pdf(file_path)
        elif file_extension == '.csv':
            text = extract_text_from_csv(file_path)
        elif file_extension in ['.xls', '.xlsx']:
            text = extract_text_from_excel(file_path)
        else:
            return jsonify({"status": "error", "message": "Unsupported file type"}), 400

        # Preprocess the extracted text
        text = preprocess_text(text)
        if not text:
            return jsonify({"status": "error", "message": "Failed to extract text"}), 500

        # Perform NER on the text
        ner_results = perform_ner(text)

        # Log NER results
        logging.info(f"NER Results for {file_name_pattern}: {ner_results}")

        # Check if the file pattern has been processed before
        stored_ner = get_ner_from_db(file_name_pattern)
        
        if stored_ner:
            # Compare the new NER results with the stored ones
            similarity_percentage = ner_similarity(stored_ner, ner_results)

            if similarity_percentage >= 90:
                return jsonify({"status": "valid", "similarity_percentage": similarity_percentage}), 200
            else:
                return jsonify({"status": "invalid", "similarity_percentage": similarity_percentage}), 200
        else:
            # Store the NER results for future comparison in PostgreSQL
            store_ner_in_db(file_name_pattern, ner_results)

            # Train the spaCy model with the new data in the background
            training_data = {
                "text": text,
                "entities": ner_results  # Passing ner_results directly as entities
            }
            train_in_background(training_data)

            return jsonify({"status": "stored", "message": "New NER result stored and model training started"}), 201

    except Exception as e:
        logging.error(f"Error during NER processing: {str(e)}")
        return jsonify({"status": "error", "message": str(e)}), 500

# Run the Flask app
if __name__ == '__main__':
    app.run(debug=True)
